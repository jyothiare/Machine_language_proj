{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "# Bag of words practice\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"C:/JBasineni/Mach_learn/ud120-projects-master/ud120-projects-master/tools/\")\n",
    "sys.path.append('C:/JBasineni/Mach_learn/ud120-projects-master/ud120-projects-master/choose_your_own')\n",
    "sys.path.append('C:/JBasineni/Mach_learn/ud120-projects-master/ud120-projects-master/datasets_questions')\n",
    "\n",
    "import os\n",
    "os.chdir('C:/JBasineni/Mach_learn/ud120-projects-master/ud120-projects-master/text_learning')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "string1 = '''Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec pharetra ornare lacinia. \n",
    "Cras et facilisis sem. Mauris fermentum nec dui at aliquam. Mauris ante justo, aliquam \n",
    "eget facilisis ac, elementum eget dolor. Nulla lacinia ac lorem vel tempus. Vivamus condimentum \n",
    "vulputate semper. Nullam dictum, mauris dignissim auctor suscipit, dui turpis elementum turpis, \n",
    "et tempor urna diam sed nisl. Vestibulum mollis quam at nisl egestas tincidunt. Nullam ut nibh \n",
    "fringilla, ultricies neque vel, tempor erat. Phasellus vel sem vitae ligula viverra luctus hendrerit \n",
    "ac quam. Vivamus dignissim, diam sed porttitor lacinia, elit erat lobortis tellus, nec euismod dolor turpis a leo.'''\n",
    "\n",
    "string2 = '''Mauris felis nibh, tempor ac pulvinar sed, feugiat at neque. In enim elit, venenatis nec magna eu, vestibulum \n",
    "lobortis libero. Nullam scelerisque pulvinar ex consectetur tempor. Morbi congue quis leo auctor facilisis. Mauris \n",
    "et diam ultricies, interdum nisi ac, condimentum turpis. Donec a risus sed dolor blandit ultrices. Nulla ac ultrices \n",
    "elit. Nulla dictum metus tortor, in sollicitudin enim rutrum id. Nulla magna felis, molestie vel odio et, congue ornare \n",
    "nisi. Nunc molestie, mi non blandit sodales, neque diam varius quam, nec tempus metus neque vel nibh. Morbi sit amet \n",
    "sapien nec ipsum feugiat lacinia ut eu orci. Mauris id enim tincidunt, aliquet augue quis, vehicula libero. Donec eu \n",
    "laoreet nibh.'''\n",
    "\n",
    "string3 = '''Donec urna massa, faucibus et interdum id, facilisis interdum augue. Nunc enim nulla, tristique sit amet velit \n",
    "in, lacinia mollis mi. Nullam malesuada felis sed libero porttitor dapibus ut quis dolor. Curabitur vitae neque \n",
    "at arcu condimentum suscipit. Quisque sollicitudin est a elit sollicitudin, ac consectetur nisi blandit. Vestibulum \n",
    "rhoncus viverra orci, et porta purus maximus quis. Pellentesque consectetur velit eget orci rutrum mollis. Nullam \n",
    "condimentum vehicula ante at dignissim. Curabitur gravida, lorem in vehicula gravida, enim arcu semper nulla, \n",
    "id tempor magna eros vel tortor.'''\n",
    "\n",
    "email_list = [string1, string2, string3]\n",
    "\n",
    "bag_of_words = vectorizer.fit_transform(email_list)\n",
    "\n",
    "print (vectorizer.vocabulary_.get('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 179\n"
     ]
    }
   ],
   "source": [
    "#Getting no of stop words from nltk\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "print ('Number of stopwords: {0}'.format(len(sw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respons\n",
      "respons\n",
      "unrespons\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Stemming with nltk\n",
    "\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "print (stemmer.stem('responsiveness'))\n",
    "print (stemmer.stem('responsivity'))\n",
    "print (stemmer.stem('unresponsive'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hi Everyone  If you can read this message youre properly using parseOutText  Please proceed to the next part of the project\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Warming up with Parse out text\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "       # text.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "        text_string = content[1].translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "       # text_string = content[1].translate(str.maketrans(\"\", \"\"))\n",
    "\n",
    "        words = text_string\n",
    "        \n",
    "    return words\n",
    "    \n",
    "\n",
    "ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "text = parseOutText(ff)\n",
    "print (text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project\n"
     ]
    }
   ],
   "source": [
    "#PRactice stemming \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def parseOutText(f):\n",
    "    '''\n",
    "    Input: a file containing text\n",
    "    \n",
    "    Output: the stemmed words in the input text, all separated by a single space\n",
    "    '''\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    \n",
    "    # the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # the string of words\n",
    "    words = \"\"\n",
    "    \n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        #text_string = content[1].translate(str.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        for word in text_string.split():\n",
    "            # stem the word and add it to words\n",
    "            words += stemmer.stem(word) + ' '       \n",
    "        \n",
    "    return words[:-1]\n",
    "    \n",
    "\n",
    "ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "text = parseOutText(ff)\n",
    "print (text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "checking for nltk\n",
      "checking for numpy\n",
      "checking for scipy\n",
      "checking for sklearn\n",
      "downloading the Enron dataset (this may take a while)\n",
      "to check on progress, you can cd up one level, then execute <ls -lthr>\n",
      "Enron dataset should be last item on the list, along with its current size\n",
      "download will complete at about 423 MB\n",
      "download complete!\n",
      "unzipping Enron dataset (this may take a while)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'enron_mail_20150507.tar.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-fcf7bce409ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"..\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mtfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"enron_mail_20150507.tar.gz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r:gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mtfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[0;32m   1584\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1585\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown compression type %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcomptype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1586\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;34m\"|\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\tarfile.py\u001b[0m in \u001b[0;36mgzopen\u001b[1;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[0;32m   1631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1633\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1634\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'enron_mail_20150507.tar.gz'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "print()\n",
    "print (\"checking for nltk\")\n",
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    print (\"you should install nltk before continuing\")\n",
    "\n",
    "print (\"checking for numpy\")\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    print (\"you should install numpy before continuing\")\n",
    "\n",
    "print (\"checking for scipy\")\n",
    "try:\n",
    "    import scipy\n",
    "except:\n",
    "    print (\"you should install scipy before continuing\")\n",
    "\n",
    "print (\"checking for sklearn\")\n",
    "try:\n",
    "    import sklearn\n",
    "except:\n",
    "    print (\"you should install sklearn before continuing\")\n",
    "\n",
    "print\n",
    "print (\"downloading the Enron dataset (this may take a while)\")\n",
    "print (\"to check on progress, you can cd up one level, then execute <ls -lthr>\")\n",
    "print (\"Enron dataset should be last item on the list, along with its current size\")\n",
    "print (\"download will complete at about 423 MB\")\n",
    "import urllib\n",
    "url = \"https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tar.gz\"\n",
    "#urllib.urlretrieve(url, filename=\"../enron_mail_20150507.tar.gz\")\n",
    "urllib.request.urlretrieve(url)\n",
    "print (\"download complete!\")\n",
    "\n",
    "\n",
    "print\n",
    "print (\"unzipping Enron dataset (this may take a while)\")\n",
    "import tarfile\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "tfile = tarfile.open(\"enron_mail_20150507.tar.gz\", \"r:gz\")\n",
    "tfile.extractall(\".\")\n",
    "\n",
    "print (\"you're ready to go!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\maildir/bailey-s/deleted_items/101.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-ecbc67af6e42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                 \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0memail\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m                     \u001b[1;31m### use parseOutText to extract the text from the opened email\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparseOutText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\maildir/bailey-s/deleted_items/101.'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "sw = [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "with open(\"from_sara.txt\", \"r\") as from_sara, open(\"from_chris.txt\", \"r\") as from_chris:\n",
    "\n",
    "    from_data = []\n",
    "    word_data = []\n",
    "\n",
    "    ### temp_counter is a way to speed up the development--there are\n",
    "    ### thousands of emails from Sara and Chris, so running over all of them\n",
    "    ### can take a long time\n",
    "    ### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "    ### can iterate your modifications quicker\n",
    "    temp_counter = 0\n",
    "\n",
    "\n",
    "    for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "        for path in from_person:\n",
    "            ### only look at first 200 emails when developing\n",
    "            ### once everything is working, remove this line to run over full dataset\n",
    "            \n",
    "            #temp_counter += 1\n",
    "            if temp_counter < 200:\n",
    "                path = os.path.join('..', path[:-1])\n",
    "\n",
    "                with open(path, 'rb') as email:\n",
    "                    ### use parseOutText to extract the text from the opened email\n",
    "                    text = parseOutText(email)\n",
    "\n",
    "                    ### use str.replace() to remove any instances of the words\n",
    "                    ### [\"sara\", \"shackleton \", \"chris\", \"germani\"]\n",
    "                    for word in sw:\n",
    "                        if(word in text):\n",
    "                            text = text.replace(word, \"\")\n",
    "\n",
    "                    ### append the text to word_data\n",
    "                    word_data.append(text.replace('\\n',' ').strip())\n",
    "\n",
    "                    ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "                    if name=='sara':\n",
    "                        from_data.append(0)\n",
    "                    else:\n",
    "                        from_data.append(1)\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
    "\n",
    "print (word_data[152])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
